"""
Script Description:
-------------------
This script processes sequences of hand images obtained from Azure Blob Storage and extracts features
using precomputed hand keypoints loaded from CSV files. The processing steps are as follows:

1. For each valid image (i.e. one with a valid keypoint row from the CSV), the image is normalized by
   rotating it based on the vector from keypoint 0 to keypoint 9 (from the keypoints CSV files generated by MediaPipe).
   If the handedness is "left" (case-insensitive), the image (and keypoints) are mirrored.
2. Keypoints are extracted from the normalized images using Harris corner detection and SIFT.
3. Optical flow is computed between consecutive frames for the extracted keypoints.
   In addition, SIFT keypoint detection on the next frame is used to validate the optical flow:
   if a predicted keypoint position drifts too far (threshold = 20 pixels) from its closest SIFT estimate,
   the SIFT location is used for that keypoint; otherwise, the optical flow prediction is used.
4. Finally, PCA is applied to reduce the keypoints (with their corresponding optical flow and grayscale intensity)
   to the top N keypoints (default 50).

Only images with valid keypoint rows (as provided in the precomputed CSV files) are processed.
All major processing steps include error print statements for troubleshooting.
"""

import numpy as np
import pandas as pd
import cv2
from sklearn.decomposition import PCA
from azure.storage.blob import BlobServiceClient

class SequenceDataProcessor:
    def __init__(self, n_points=50):
        if not isinstance(n_points, int) or n_points <= 0:
            raise ValueError("n_points must be a positive integer.")
        self.n_points = n_points

    def get_blob_lists(self, container_name, connection_string):
        """
        Connects to the given container, retrieves all JPEG blob names,
        and returns a dictionary grouping blob names by category.
        The grouping is inferred from the container name:
          - "img-adv-train"   -> "training_advanced"
          - "img-beg-train"   -> "training_beginner"
          - "img-adv-holdout" -> "holdout_advanced"
          - "img-beg-holdout" -> "holdout_beginner"
        """
        blob_lists = {
            "training_advanced": [],
            "training_beginner": [],
            "holdout_advanced": [],
            "holdout_beginner": []
        }
        
        try:
            blob_service_client = BlobServiceClient.from_connection_string(connection_string)
            container_client = blob_service_client.get_container_client(container_name)
        except Exception as e:
            print(f"Error connecting to container '{container_name}': {e}")
            return blob_lists

        try:
            # Retrieve and sort JPEG blob names.
            blobs = container_client.list_blobs()
            blob_names = [blob.name for blob in blobs if blob.name.lower().endswith('.jpg')]
            blob_names.sort()
        except Exception as e:
            print(f"Error listing blobs in container '{container_name}': {e}")
            return blob_lists

        lower_name = container_name.lower()
        if "adv" in lower_name:
            if "holdout" in lower_name:
                blob_lists["holdout_advanced"] = blob_names
            else:
                blob_lists["training_advanced"] = blob_names
        elif "beg" in lower_name:
            if "holdout" in lower_name:
                blob_lists["holdout_beginner"] = blob_names
            else:
                blob_lists["training_beginner"] = blob_names
        else:
            blob_lists["training_advanced"] = blob_names

        return blob_lists

    def normalize_images(self, image, keypoints, handedness):
        """
        Normalize image by rotating it based on keypoints.
        Then, mirror the image (and adjust keypoints) only if handedness is "left" (case-insensitive).
        Returns the rotated (and possibly mirrored) image and transformed keypoints.
        """
        if image is None or keypoints is None:
            raise ValueError("Image and keypoints must be provided to normalize_images.")
        if len(keypoints) < 10:
            raise ValueError("Insufficient keypoints provided for normalization.")
            
        
        # This executes a rotation on the x y plane
        # Could also include a depth z coord rotation to ensure hand is parallel with x y plane
        # however processing time would go up
        # Use only the first two coordinates (x, y) for the 2D transformation.
        keypoints_2d = np.array(keypoints)[:, :2]
        vector = keypoints_2d[9] - keypoints_2d[0]
        angle = np.degrees(np.arctan2(vector[0], vector[1]))
        h, w = image.shape[:2]
        center = (w // 2, h // 2)
        M = cv2.getRotationMatrix2D(center, -angle, 1.0)
        rotated_image = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_LINEAR,
                                        borderMode=cv2.BORDER_CONSTANT, borderValue=0)
        ones = np.ones((keypoints_2d.shape[0], 1))
        keypoints_homog = np.hstack([keypoints_2d, ones])
        rotated_keypoints = M.dot(keypoints_homog.T).T
        
        if handedness.lower() == "left":
            rotated_image = cv2.flip(rotated_image, 1)
            rotated_keypoints[:, 0] = w - rotated_keypoints[:, 0]
            
        return rotated_image, rotated_keypoints

    def match_consecutive_frames(self, prev_img, next_img):
        """
        Perform frame-to-frame matching using relaxed Harris detection and SIFT.
        First resizes images to control memory usage, then extracts and matches features.
        Returns matched keypoints (from the previous image) as a numpy array of shape (N, 2).
        """
        # Convert to grayscale first to save memory during resize
        prev_gray = cv2.cvtColor(prev_img, cv2.COLOR_BGR2GRAY) if prev_img.ndim == 3 else prev_img.copy()
        next_gray = cv2.cvtColor(next_img, cv2.COLOR_BGR2GRAY) if next_img.ndim == 3 else next_img.copy()
        
        # Resize images to reduce memory requirements
        max_dim = 512  # Maximum dimension for processing
        
        h_prev, w_prev = prev_gray.shape[:2]
        h_next, w_next = next_gray.shape[:2]
        
        # Calculate scaling factors
        scale_prev = min(max_dim / w_prev, max_dim / h_prev)
        scale_next = min(max_dim / w_next, max_dim / h_next)
        
        # Store original scale for later correction
        orig_scale = 1.0 / scale_prev
        
        # Resize only if needed
        if scale_prev < 1:
            new_size = (int(w_prev * scale_prev), int(h_prev * scale_prev))
            prev_gray = cv2.resize(prev_gray, new_size, interpolation=cv2.INTER_AREA)
        
        if scale_next < 1:
            new_size = (int(w_next * scale_next), int(h_next * scale_next))
            next_gray = cv2.resize(next_gray, new_size, interpolation=cv2.INTER_AREA)
        
        try:
            # Harris corner detection with relaxed threshold
            harris = cv2.cornerHarris(np.float32(prev_gray), blockSize=2, ksize=3, k=0.04)
            harris = cv2.dilate(harris, None)
            ret, harris_thresh = cv2.threshold(harris, 0.005 * harris.max(), 255, 0)
            harris_thresh = np.uint8(harris_thresh)
            ret, labels, stats, centroids = cv2.connectedComponentsWithStats(harris_thresh)
            candidate_points = np.unique(np.round(centroids), axis=0)
            candidate_kps = [cv2.KeyPoint(x=float(pt[0]), y=float(pt[1]), size=3) for pt in candidate_points]
            
            # Use fewer features to reduce memory usage
            sift = cv2.SIFT_create(nfeatures=500)
            kp_prev, des_prev = sift.compute(prev_gray, candidate_kps)
            kp_next, des_next = sift.detectAndCompute(next_gray, None)
            
            if des_prev is None or des_next is None or len(kp_prev) == 0 or len(kp_next) == 0:
                return np.empty((0, 2), dtype=np.int32)
            
            bf = cv2.BFMatcher()
            matches = bf.knnMatch(des_prev, des_next, k=2)
            good_matches = []
            for m_n in matches:
                if len(m_n) < 2:
                    continue
                m, n = m_n
                if m.distance < 0.85 * n.distance:
                    good_matches.append(m)
                    
            matched_pts = np.array([kp_prev[m.queryIdx].pt for m in good_matches], dtype=np.float32)
            
            # Scale points back to original image size
            if scale_prev < 1:
                matched_pts *= orig_scale
                
            return matched_pts.astype(np.int32)
            
        except cv2.error as e:
            print(f"OpenCV error: {e}")
            # Fallback to simpler detection on even smaller image if SIFT fails
            try:
                if prev_gray.shape[0] > 256 or prev_gray.shape[1] > 256:
                    prev_gray = cv2.resize(prev_gray, (256, 256), interpolation=cv2.INTER_AREA)
                    next_gray = cv2.resize(next_gray, (256, 256), interpolation=cv2.INTER_AREA)
                    
                fast = cv2.FastFeatureDetector_create(threshold=20)
                kp_prev = fast.detect(prev_gray, None)
                kp_next = fast.detect(next_gray, None)
                
                brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()
                kp_prev, des_prev = brief.compute(prev_gray, kp_prev)
                kp_next, des_next = brief.compute(next_gray, kp_next)
                
                if des_prev is None or des_next is None or len(kp_prev) == 0 or len(kp_next) == 0:
                    return np.empty((0, 2), dtype=np.int32)
                
                bf = cv2.BFMatcher(cv2.NORM_HAMMING)
                matches = bf.knnMatch(des_prev, des_next, k=2) if len(des_prev) > 0 and len(des_next) > 0 else []
                
                good_matches = []
                for m_n in matches:
                    if len(m_n) < 2:
                        continue
                    m, n = m_n
                    if m.distance < 0.8 * n.distance:
                        good_matches.append(m)
                        
                matched_pts = np.array([kp_prev[m.queryIdx].pt for m in good_matches], dtype=np.float32)
                
                # Scale points back to original image size
                matched_pts *= (w_prev / 256.0)
                    
                return matched_pts.astype(np.int32)
            except Exception as e2:
                print(f"Fallback feature detection also failed: {e2}")
                return np.empty((0, 2), dtype=np.int32)
    
    def keypoint_detector_sequence(self, sequence):
        """
        For a sequence of grayscale images, perform frame-to-frame matching.
        Returns a combined numpy array of all matched keypoints.
        Also prints the number of keypoints originally found in each frame.
        """
        all_matches = []
        for i in range(len(sequence) - 1):
            matched = self.match_consecutive_frames(sequence[i], sequence[i+1])
            print(f"Frame pair {i} -> {i+1}: {matched.shape[0]} matched keypoints found.")
            if matched.shape[0] > 0:
                all_matches.append(matched)
        if len(all_matches) > 0:
            combined = np.vstack(all_matches)
            return combined
        else:
            return np.empty((0, 2), dtype=np.int32)
    
    def optical_flow(self, prev_image, next_image, prev_keypoints):
        """
        Calculate optical flow between two frames for given keypoints using cv2.calcOpticalFlowPyrLK.
        Uses improved parameters for increased robustness.
        Returns:
        - flow_full: an array of shape (N, 2) with displacement vectors (zero-filled for failed points),
        - status: a tracking status array.
        """
        if prev_image is None or next_image is None:
            raise ValueError("Both previous and next images must be provided.")
        if prev_keypoints is None or len(prev_keypoints) == 0:
            raise ValueError("Non-empty previous keypoints must be provided.")

        prev_gray = cv2.cvtColor(prev_image, cv2.COLOR_BGR2GRAY) if prev_image.ndim == 3 else prev_image.copy()
        next_gray = cv2.cvtColor(next_image, cv2.COLOR_BGR2GRAY) if next_image.ndim == 3 else next_image.copy()
        prev_pts = np.array(prev_keypoints, dtype=np.float32).reshape(-1, 1, 2)

        lk_params = dict(
            winSize=(21, 21),
            maxLevel=3,
            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01),
            minEigThreshold=0.001
        )
        next_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, next_gray, prev_pts, None, **lk_params)

        if next_pts is None or status is None:
            raise RuntimeError("Optical flow calculation failed.")

        valid_indices = np.where(status.flatten() == 1)[0]

        if np.sum(valid_indices) < 10:
            acceptable_err = err < np.percentile(err, 80)
            valid_indices = np.where((status.flatten() == 1) | acceptable_err.flatten())[0]

        flow = next_pts[valid_indices].reshape(-1, 2) - prev_pts[valid_indices].reshape(-1, 2)

        # Initialize a full-sized flow array with zeros
        flow_full = np.zeros((prev_keypoints.shape[0], 2), dtype=np.float32)

        # Ensure matching shape before assignment
        if flow.shape[0] == valid_indices.shape[0]:
            flow_full[valid_indices, :] = flow
        elif flow.shape[0] > valid_indices.shape[0]:
            flow_full[valid_indices, :] = flow[:valid_indices.shape[0]]  # Trim extra flow vectors
        elif flow.shape[0] < valid_indices.shape[0]:
            print(f"Warning: Fewer flow vectors ({flow.shape[0]}) than valid indices ({valid_indices.shape[0]}).")
            valid_indices = valid_indices[:flow.shape[0]]  # Trim valid indices to match flow
            flow_full[valid_indices, :] = flow

        return flow_full, status


def row_to_keypoints(row):
    """
    Converts a dictionary (CSV row) with columns "lm_0_x", "lm_0_y", "lm_0_z", ..., "lm_20_x", "lm_20_y", "lm_20_z"
    into a numpy array of shape (21, 3). If any coordinate is missing (NaN), returns None.
    """
    # Quick check if any keypoint is missing
    if any(pd.isna(row.get(f'lm_{i}_x', np.nan)) for i in range(21)):
        return None
    
    # Create the keypoints array directly from the row
    keypoints = np.array([
        [row.get(f'lm_{i}_x', np.nan), row.get(f'lm_{i}_y', np.nan), row.get(f'lm_{i}_z', np.nan)]
        for i in range(21)
    ])
    
    return keypoints

def group_into_sequences(lst, sequence_length=20):
    return [lst[i:i+sequence_length] for i in range(0, len(lst), sequence_length)]

def download_sequences(blob_sequences, container_name, connection_string, kp_sequences):
    """
    Downloads multiple image sequences in one go, ensuring each sequence consists of 20 frames.
    
    Args:
        blob_sequences: List of lists, where each inner list contains 20 blob names.
        container_name: Azure container name.
        connection_string: Azure connection string.
        kp_sequences: List of lists, where each inner list contains 20 keypoint rows.

    Returns:
        List of sequences, where each sequence contains 20 tuples of (image, keypoints, handedness).
    """
    print(f"Downloading {len(blob_sequences)} sequences (each with 20 frames) from container: {container_name}")

    sequences = []
    
    try:
        blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        container_client = blob_service_client.get_container_client(container_name)
    except Exception as e:
        print(f"Error: Failed to connect to container {container_name}: {e}")
        return sequences

    # Flatten all blob names to download in one batch
    all_blob_names = [blob_name for sequence in blob_sequences for blob_name in sequence]
    all_kp_rows = [kp_row for sequence in kp_sequences for kp_row in sequence]

    frames_dict = {}

    # Download all blobs in one batch
    for i, blob_name in enumerate(all_blob_names):
        try:
            blob_client = container_client.get_blob_client(blob_name)
            image_bytes = blob_client.download_blob().readall()
            np_arr = np.frombuffer(image_bytes, np.uint8)
            img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)

            if img is None:
                print(f"Warning: Unable to decode image {blob_name}")
                continue

            kp_row = all_kp_rows[i]

            if not isinstance(kp_row, dict):
                print(f"Warning: kp_row for {blob_name} is not a dictionary-like structure")
                continue

            keypoints = row_to_keypoints(kp_row)
            handedness = kp_row.get("hand", np.nan) if isinstance(kp_row, dict) else np.nan

            if keypoints is None:
                print(f"Warning: Missing keypoints for image {blob_name}")
                continue

            frames_dict[blob_name] = (img, keypoints, handedness)

        except Exception as e:
            print(f"Error downloading or decoding blob {blob_name}: {e}")
            continue

    # Reconstruct sequences based on the original order
    for sequence in blob_sequences:
        frames = [frames_dict[blob] for blob in sequence if blob in frames_dict]

        if len(frames) == 20:
            sequences.append(frames)

    print(f"Successfully downloaded {len(sequences)} full sequences")
    return sequences

if __name__ == "__main__":
    processor = SequenceDataProcessor(n_points=50)

    # Load Precomputed Keypoint CSVs
    train_adv_kps_csv = "test_kps_adv_train_sampled.csv"
    train_beg_kps_csv = "test_kps_beg_train_sampled.csv"
    holdout_adv_kps_csv = "test_kps_adv_holdout_sampled.csv"
    holdout_beg_kps_csv = "test_kps_beg_holdout_sampled.csv"

    df_train_adv_kp = pd.read_csv(train_adv_kps_csv)
    df_train_beg_kp = pd.read_csv(train_beg_kps_csv)
    df_hold_adv_kp = pd.read_csv(holdout_adv_kps_csv)
    df_hold_beg_kp = pd.read_csv(holdout_beg_kps_csv)
    
    kp_rows_train_adv = df_train_adv_kp.to_dict('records')
    kp_rows_train_beg = df_train_beg_kp.to_dict('records')
    kp_rows_hold_adv = df_hold_adv_kp.to_dict('records')
    kp_rows_hold_beg = df_hold_beg_kp.to_dict('records')
    
    # Blob Storage Settings
    train_adv_container = "img-adv-train"
    train_beg_container = "img-beg-train"
    holdout_adv_container = "img-adv-holdout"
    holdout_beg_container = "img-beg-holdout"
    connection_string = "DefaultEndpointsProtocol=https;AccountName=cs131finalproj;AccountKey=mdUYnEKOv/IkY/axGC0plybXPdPtoXRF6NsTFyrstcxin1Wy3iu0S1Ofb6jpZGUohe1R8uWPQCbt+AStWk8lTQ==;EndpointSuffix=core.windows.net"

    blob_lists_train_adv = processor.get_blob_lists(train_adv_container, connection_string)
    blob_lists_train_beg = processor.get_blob_lists(train_beg_container, connection_string)
    blob_lists_hold_adv  = processor.get_blob_lists(holdout_adv_container, connection_string)
    blob_lists_hold_beg  = processor.get_blob_lists(holdout_beg_container, connection_string)
    
    sequences_train_adv = group_into_sequences(blob_lists_train_adv["training_advanced"])
    sequences_train_beg = group_into_sequences(blob_lists_train_beg["training_beginner"])
    sequences_hold_adv  = group_into_sequences(blob_lists_hold_adv["holdout_advanced"])
    sequences_hold_beg  = group_into_sequences(blob_lists_hold_beg["holdout_beginner"])
    
    kp_seq_train_adv = group_into_sequences(kp_rows_train_adv, sequence_length=20)
    kp_seq_train_beg = group_into_sequences(kp_rows_train_beg, sequence_length=20)
    kp_seq_hold_adv = group_into_sequences(kp_rows_hold_adv, sequence_length=20)
    kp_seq_hold_beg = group_into_sequences(kp_rows_hold_beg, sequence_length=20)
    
    # Ensure we only download sequences up to len(kp_rows) // 20
    num_sequences_train_adv = len(kp_rows_train_adv) // 20
    num_sequences_train_beg = len(kp_rows_train_beg) // 20
    num_sequences_hold_adv = len(kp_rows_hold_adv) // 20
    num_sequences_hold_beg = len(kp_rows_hold_beg) // 20

    downloaded_train_adv = download_sequences(
        sequences_train_adv[:num_sequences_train_adv],
        train_adv_container,
        connection_string,
        kp_seq_train_adv[:num_sequences_train_adv]
    )
    print("DONE DOWNLOADING TRAINING ADVANCED")

    downloaded_train_beg = download_sequences(
        sequences_train_beg[:num_sequences_train_beg],
        train_beg_container,
        connection_string,
        kp_seq_train_beg[:num_sequences_train_beg]
    )
    print("DONE DOWNLOADING TRAINING BEGINNER")

    downloaded_hold_adv = download_sequences(
        sequences_hold_adv[:num_sequences_hold_adv],
        holdout_adv_container,
        connection_string,
        kp_seq_hold_adv[:num_sequences_hold_adv]
    )
    print("DONE DOWNLOADING HOLDOUT ADVANCED")

    downloaded_hold_beg = download_sequences(
        sequences_hold_beg[:num_sequences_hold_beg],
        holdout_beg_container,
        connection_string,
        kp_seq_hold_beg[:num_sequences_hold_beg]
    )
    print("DONE DOWNLOADING HOLDOUT BEGINNER")

    
    print("GOT PAST DOWNLOADS")
    
    def process_sequence(sequence):
        """
        Process a single 20-frame sequence to compute per-frame pair features.
        Each frame in the sequence is a tuple: (image, keypoints, handedness).
        This function extracts:
        - Keypoint coordinates (x, y)
        - Optical flow values (flow_x, flow_y)
        - Intensity at each keypoint

        Returns a list of feature vectors, where each row contains:
        [kp1_x, kp1_y, intensity1, flow1_x, flow1_y, ..., kpN_x, kpN_y, intensityN, flowN_x, flowN_y]
        """
        normalized_sequence = []
        for item in sequence:
            image, keypoints, handedness = item
            try:
                norm_img, _ = processor.normalize_images(image, keypoints, handedness)
                normalized_sequence.append(norm_img)
            except Exception as e:
                print("Error during image normalization:", e)

        valid_norm = [img for img in normalized_sequence if img is not None and not np.isnan(img).all()]
        if len(valid_norm) < 2:
            return []  # Not enough valid frames

        processed_gray = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if img.ndim == 3 else img for img in valid_norm]
        matched_keypoints = processor.keypoint_detector_sequence(processed_gray)

        if matched_keypoints.shape[0] == 0:
            print("No matched keypoints found in sequence.")
            return []

        rows = []
        for i in range(len(processed_gray) - 1):
            current_frame = processed_gray[i]
            next_frame = processed_gray[i + 1]

            indices_x = np.clip(matched_keypoints[:, 0].astype(np.int32), 0, current_frame.shape[1] - 1)
            indices_y = np.clip(matched_keypoints[:, 1].astype(np.int32), 0, current_frame.shape[0] - 1)
            intensities = current_frame[indices_y, indices_x]

            try:
                # Get optical flow directly, don't use the returned flow_full from the function
                # as we'll create our own flow_full array
                flow_result, status = processor.optical_flow(current_frame, next_frame, matched_keypoints)
                
                # Create flow_full array
                flow_full = np.zeros((matched_keypoints.shape[0], 2), dtype=np.float32)
                
                # Get indices of valid flow vectors
                valid_indices = np.where(status.flatten() == 1)[0]
                
                # Ensure we only use valid flow vectors that correspond to valid indices
                # This is important to avoid the shape mismatch error
                if len(valid_indices) > 0:
                    # Take only the subset of flow that corresponds to valid indices
                    valid_flow = flow_result[valid_indices]
                    flow_full[valid_indices] = valid_flow
                
            except Exception as e:
                print(f"Error computing optical flow: {e}")
                flow_full = np.zeros((matched_keypoints.shape[0], 2), dtype=np.float32)

            # Format the feature vector for CSV storage
            row_features = np.hstack([matched_keypoints, intensities.reshape(-1, 1), flow_full]).flatten().tolist()
            rows.append(row_features)

        return rows
    
    def process_sequences(sequences):
        """
        Process all sequences and collect results into a list.
        Each row represents one frame in a sequence.
        """
        all_rows = []
        for seq in sequences:
            rows = process_sequence(seq)
            all_rows.extend(rows)
        return all_rows
    
    from sklearn.preprocessing import StandardScaler
    
    rows_train_adv = process_sequences(downloaded_train_adv)
    print("DONE PROCESSING TRAIN ADVANCED")
    rows_train_beg = process_sequences(downloaded_train_beg)
    print("DONE PROCESSING TRAIN BEGINNER")
    rows_hold_adv  = process_sequences(downloaded_hold_adv)
    print("DONE PROCESSING HOLDOUT ADVANCED")
    rows_hold_beg  = process_sequences(downloaded_hold_beg)
    print("DONE PROCESSING HOLDOUT BEGINNER")

    df_train_adv = pd.DataFrame(rows_train_adv)
    df_train_beg = pd.DataFrame(rows_train_beg)
    df_hold_adv = pd.DataFrame(rows_hold_adv)
    df_hold_beg = pd.DataFrame(rows_hold_beg)
    
    df_train_adv.to_csv("train_advanced_features.csv", index=False)
    df_train_beg.to_csv("train_beginner_features.csv", index=False)
    df_hold_adv.to_csv("holdout_advanced_features.csv", index=False)
    df_hold_beg.to_csv("holdout_beginner_features.csv", index=False)
    
    print("CSV files generated for all groups.")